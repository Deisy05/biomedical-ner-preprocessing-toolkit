#!/usr/bin/env python3
"""
Este script procesa todas las carpetas y divide por archivos completos 
para armar el train, el valid y el test, en lugar de tomar l√≠neas aleatoriamente.
Esto con el fin de no da√±ar el contexto de una historia m√©dica completa
"""

import os
import json
import random
import logging
from pathlib import Path

# Configurar el sistema de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_split.log'),
        logging.StreamHandler()
    ]
)

def process_folders():
    """Procesa todas las carpetas y divide por archivos completos en lugar de l√≠neas."""
    # Crear el directorio de salida si no existe
    output_dir = Path('output_data')
    output_dir.mkdir(exist_ok=True)
    
    # Lista para almacenar informaci√≥n de todos los archivos
    all_files_info = []
    error_count = 0
    
    # Carpeta a omitir
    folder_to_skip = "nuevos_andres check 2"
    
    # Obtener todas las carpetas en el directorio actual, excluyendo la carpeta a omitir
    folders = [f for f in os.listdir('.') if os.path.isdir(f) and not f.startswith('.') 
              and f != 'output_data' and f != folder_to_skip]
    
    if not folders:
        logging.warning("No se encontraron carpetas para procesar en el directorio actual.")
        return
    
    logging.info(f"Procesando {len(folders)} carpetas: {', '.join(folders)}")
    logging.info(f"Omitiendo la carpeta: {folder_to_skip}")
    
    # Recopilar informaci√≥n de todos los archivos JSON
    for folder in folders:
        try:
            folder_path = Path(folder)
            json_files = list(folder_path.glob('*.json'))
            
            if not json_files:
                logging.info(f"No se encontraron archivos JSON en la carpeta {folder}")
                continue
            
            logging.info(f"Encontrados {len(json_files)} archivos JSON en la carpeta {folder}")
            
            # Validar cada archivo y contar sus l√≠neas
            for json_file in json_files:
                try:
                    file_data = []
                    with open(json_file, 'r', encoding='utf-8') as f:
                        for line_number, line in enumerate(f, 1):
                            try:
                                line = line.strip()
                                if line:  # Ignorar l√≠neas vac√≠as
                                    json_data = json.loads(line)
                                    # Verificar que el formato es correcto
                                    if "sentencia" in json_data and "tag" in json_data:
                                        file_data.append(json_data)
                                    else:
                                        logging.warning(f"Formato incorrecto en {json_file}, l√≠nea {line_number}")
                                        error_count += 1
                            except json.JSONDecodeError:
                                logging.error(f"Error al decodificar JSON en {json_file}, l√≠nea {line_number}")
                                error_count += 1
                    
                    # Solo agregar archivos que tengan datos v√°lidos
                    if file_data:
                        all_files_info.append({
                            'file_path': json_file,
                            'data': file_data,
                            'count': len(file_data)
                        })
                        logging.info(f"Archivo v√°lido: {json_file} con {len(file_data)} l√≠neas")
                    else:
                        logging.warning(f"Archivo sin datos v√°lidos: {json_file}")
                        
                except Exception as e:
                    logging.error(f"Error al procesar el archivo {json_file}: {str(e)}")
                    error_count += 1
        except Exception as e:
            logging.error(f"Error al procesar la carpeta {folder}: {str(e)}")
            error_count += 1
    
    # Verificar si se encontraron archivos v√°lidos
    if not all_files_info:
        logging.error("No se encontraron archivos v√°lidos para procesar.")
        print("‚ùå Error: No se encontraron archivos v√°lidos para procesar.")
        return False
    
    # Mezclar los archivos aleatoriamente
    random.shuffle(all_files_info)
    
    # Calcular √≠ndices para la divisi√≥n (por n√∫mero de archivos)
    total_files = len(all_files_info)
    train_files_idx = int(0.8 * total_files)
    valid_files_idx = int(0.9 * total_files)
    
    # Dividir los archivos
    train_files = all_files_info[:train_files_idx]
    valid_files = all_files_info[train_files_idx:valid_files_idx]
    test_files = all_files_info[valid_files_idx:]
    
    # Funci√≥n auxiliar para escribir archivos y contar l√≠neas
    def write_dataset(files_list, output_filename, dataset_name):
        total_lines = 0
        file_names = []
        
        with open(output_dir / output_filename, 'w', encoding='utf-8') as f:
            for file_info in files_list:
                file_names.append(file_info['file_path'].name)
                for item in file_info['data']:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    total_lines += 1
        
        logging.info(f"Dataset {dataset_name}:")
        logging.info(f"  - {len(files_list)} archivos ({len(files_list)/total_files:.2%})")
        logging.info(f"  - {total_lines} l√≠neas totales")
        logging.info(f"  - Archivos incluidos: {', '.join(file_names[:5])}{'...' if len(file_names) > 5 else ''}")
        
        return total_lines, file_names
    
    # Guardar los datasets
    try:
        train_lines, train_file_names = write_dataset(train_files, 'train.json', 'ENTRENAMIENTO')
        valid_lines, valid_file_names = write_dataset(valid_files, 'valid.json', 'VALIDACI√ìN')
        test_lines, test_file_names = write_dataset(test_files, 'test.json', 'PRUEBA')
        
        # Calcular totales
        total_lines = train_lines + valid_lines + test_lines
        
        # Resumen final
        print("\n" + "="*60)
        print("RESUMEN DE LA DIVISI√ìN POR ARCHIVOS")
        print("="*60)
        print(f"Total de archivos procesados: {total_files}")
        print(f"Total de l√≠neas procesadas: {total_lines}")
        print(f"\nDISTRIBUCI√ìN:")
        print(f"  üü¢ TRAIN:  {len(train_files):3d} archivos ({len(train_files)/total_files:.1%}) ‚Üí {train_lines:5d} l√≠neas ({train_lines/total_lines:.1%})")
        print(f"  üü° VALID:  {len(valid_files):3d} archivos ({len(valid_files)/total_files:.1%}) ‚Üí {valid_lines:5d} l√≠neas ({valid_lines/total_lines:.1%})")
        print(f"  üî¥ TEST:   {len(test_files):3d} archivos ({len(test_files)/total_files:.1%}) ‚Üí {test_lines:5d} l√≠neas ({test_lines/total_lines:.1%})")
        print("="*60)
        
        # Guardar reporte detallado
        report_path = output_dir / 'division_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("REPORTE DETALLADO DE DIVISI√ìN POR ARCHIVOS\n")
            f.write("="*50 + "\n\n")
            
            f.write(f"ARCHIVOS DE ENTRENAMIENTO ({len(train_files)}):\n")
            for i, file_info in enumerate(train_files, 1):
                f.write(f"  {i:2d}. {file_info['file_path']} ({file_info['count']} l√≠neas)\n")
            
            f.write(f"\nARCHIVOS DE VALIDACI√ìN ({len(valid_files)}):\n")
            for i, file_info in enumerate(valid_files, 1):
                f.write(f"  {i:2d}. {file_info['file_path']} ({file_info['count']} l√≠neas)\n")
            
            f.write(f"\nARCHIVOS DE PRUEBA ({len(test_files)}):\n")
            for i, file_info in enumerate(test_files, 1):
                f.write(f"  {i:2d}. {file_info['file_path']} ({file_info['count']} l√≠neas)\n")
        
        logging.info(f"Reporte detallado guardado en: {report_path}")
        
        if error_count > 0:
            logging.warning(f"Se encontraron {error_count} errores durante el procesamiento.")
            print(f"‚ö†Ô∏è  Se encontraron {error_count} errores durante el procesamiento.")
            return False
        else:
            logging.info("Proceso completado sin errores.")
            print("‚úÖ El proceso se complet√≥ correctamente sin errores.")
            return True
            
    except Exception as e:
        logging.error(f"Error al guardar los archivos de salida: {str(e)}")
        print(f"‚ùå Error al guardar los archivos de salida: {str(e)}")
        return False

if __name__ == "__main__":
    logging.info("Iniciando proceso de divisi√≥n de datos POR ARCHIVOS")
    result = process_folders()
    if result:
        print("‚úÖ Proceso finalizado con √©xito")
    else:
        print("‚ùå Proceso finalizado con errores")
    logging.info("Proceso finalizado")
